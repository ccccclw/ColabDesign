{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ccccclw/ColabDesign/blob/main/af/examples/alphafolding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapse": true
   },
   "source": [
    "### This notebook supports\n",
    "- running iterative predictions with AlphaFold2 (monomer model 1,2) and visualization of structure predictions. For predictions that succesfully find the native state, the structure predictions before native state can possibly resemble protein folding intermediates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapse": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%%time` not found.\n"
     ]
    }
   ],
   "source": [
    "#@title setup {\"vertical-output\":true,\"form-width\":\"50%\",\"display-mode\":\"form\"}\n",
    "%%time\n",
    "import os\n",
    "if not os.path.isdir(\"params\"):\n",
    "  # get code\n",
    "  os.system(\"pip -q install git+https://github.com/ccccclw/ColabDesign.git\")\n",
    "  # for debugging\n",
    "  os.system(\"ln -s /usr/local/lib/python3.*/dist-packages/colabdesign colabdesign\")\n",
    "  # download params\n",
    "  os.system(\"mkdir params\")\n",
    "  os.system(\"apt-get install aria2 -qq\")\n",
    "  os.system(\"aria2c -q -x 16 https://storage.googleapis.com/alphafold/alphafold_params_2022-12-06.tar\")\n",
    "  os.system(\"tar -xf alphafold_params_2022-12-06.tar -C params\")\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from Bio.PDB import *\n",
    "import os, re\n",
    "from colabdesign import mk_afdesign_model, clear_mem\n",
    "from colabdesign.mpnn import mk_mpnn_model\n",
    "from colabdesign.af.alphafold.common import residue_constants\n",
    "from colabdesign.shared.protein import _np_get_cb\n",
    "from colabdesign.shared.plot import plot_pseudo_3D, make_animation, show_pdb\n",
    "import pickle\n",
    "from colabdesign import af\n",
    "from google.colab import files\n",
    "import numpy as np\n",
    "from IPython.display import HTML\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "from scipy.special import softmax\n",
    "import sys\n",
    "import tqdm.notebook\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "TQDM_BAR_FORMAT = '{l_bar}{bar}| {n_fmt}/{total_fmt} [elapsed: {elapsed} remaining: {remaining}]'\n",
    "\n",
    "##util functions\n",
    "def get_pdb(pdb_code=\"\"):\n",
    "  if pdb_code is None or pdb_code == \"\":\n",
    "    upload_dict = files.upload()\n",
    "    pdb_string = upload_dict[list(upload_dict.keys())[0]]\n",
    "    with open(\"tmp.pdb\",\"wb\") as out: out.write(pdb_string)\n",
    "    return \"tmp.pdb\"\n",
    "  elif os.path.isfile(pdb_code):\n",
    "    return pdb_code\n",
    "  elif len(pdb_code) == 4:\n",
    "    os.system(f\"wget -qnc https://files.rcsb.org/view/{pdb_code}.pdb\")\n",
    "    return f\"{pdb_code}.pdb\"\n",
    "  else:\n",
    "    os.system(f\"wget -qnc https://alphafold.ebi.ac.uk/files/AF-{pdb_code}-F1-model_v3.pdb\")\n",
    "    return f\"AF-{pdb_code}-F1-model_v3.pdb\"\n",
    "\n",
    "\n",
    "def get_dgram(positions, num_bins=39, min_bin=3.25, max_bin=50.75):\n",
    "  atom_idx = residue_constants.atom_order\n",
    "  atoms = {k:positions[...,atom_idx[k],:] for k in [\"N\",\"CA\",\"C\"]}\n",
    "  cb = _np_get_cb(**atoms, use_jax=False)\n",
    "  dist2 = np.square(cb[None,:] - cb[:,None]).sum(-1,keepdims=True)\n",
    "  lower_breaks = np.linspace(min_bin, max_bin, num_bins)\n",
    "  lower_breaks = np.square(lower_breaks)\n",
    "  upper_breaks = np.concatenate([lower_breaks[1:],np.array([1e8], dtype=jnp.float32)], axis=-1)\n",
    "  return ((dist2 > lower_breaks) * (dist2 < upper_breaks)).astype(float)\n",
    "\n",
    "def sample_gumbel(shape, eps=1e-10):                  \n",
    "  \"\"\"Sample from Gumbel(0, 1)\"\"\"\n",
    "  U = np.random.uniform(size=shape)\n",
    "  return -np.log(-np.log(U + eps) + eps)\n",
    " \n",
    "def sample_uniform(shape, eps=1e-10): \n",
    "  \"\"\"Sample from Uniform(0, 1)\"\"\"\n",
    "  U = np.random.uniform(size=shape)\n",
    "  return U + eps\n",
    " \n",
    "from colabdesign.af.alphafold.common import residue_constants\n",
    "def xyz_atom37(pdb_file):\n",
    "  \"\"\"\n",
    "  Convert atom coordinates [num_atom, 3] from xyz read from file such as pdb to atom37 format.\n",
    "  \"\"\"\n",
    "  atom37_order = residue_constants.atom_order\n",
    "  parser = PDBParser()\n",
    "  structure = parser.get_structure(\"A\", pdb_file)\n",
    "  atoms = list(structure.get_atoms())\n",
    "  length = len(list(structure.get_residues()))\n",
    "  atom37_coord = np.zeros((length, 37, 3))\n",
    "  \n",
    "  for atom in atoms:\n",
    "    atom37_index = atom37_order[atom.get_name()]\n",
    "    residue_index = atom.get_parent().id[1]\n",
    "    atom37_coord[residue_index-1][atom37_index] = atom.get_coord()\n",
    "  return atom37_coord\n",
    "\n",
    "def sequence_to_one_hot(sequence):\n",
    "    \"\"\"\n",
    "    Convert a sequence string into a one-hot encoding matrix of shape (N, 20),\n",
    "    where N is the number of residues, and 20 is the number of amino acids.\n",
    "    \n",
    "    Parameters:\n",
    "    - sequence: str, the input sequence of amino acids (e.g., \"ACDE\").\n",
    "    \n",
    "    Returns:\n",
    "    - one_hot_matrix: np.ndarray, one-hot encoding matrix of shape (N, 20).\n",
    "    \"\"\"\n",
    "    # Convert the sequence to a list of integers using aa_order dictionary\n",
    "    aa_dict = residue_constants.restype_order\n",
    "    seq_indices = [aa_dict.get(aa, -1) for aa in sequence]  # -1 for unknown AA\n",
    "    \n",
    "    # Ensure no unknown amino acids (-1) are present in the sequence\n",
    "    if any(idx == -1 for idx in seq_indices):\n",
    "        raise ValueError(\"Sequence contains invalid amino acid(s) not present in aa_order.\")\n",
    "    \n",
    "    # Create a one-hot encoding matrix\n",
    "    N = len(sequence)\n",
    "    one_hot_matrix = np.eye(20)[seq_indices]\n",
    "    \n",
    "    return one_hot_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapse": true
   },
   "outputs": [],
   "source": [
    "#@title input preparation {\"vertical-output\":true,\"form-width\":\"50%\",\"display-mode\":\"form\"}\n",
    "starting_seq = \"\" #@param {type:\"string\"}\n",
    "starting_seq = re.sub(\"[^A-Z]\", \"\", starting_seq.upper())\n",
    "##default sequence is PDB:3GB1 if no sequence is provided\n",
    "starting_seq = \"MTYKLILNGKTLKGETTTEAVDAATAEKVFKQYANDNGVDGEWTYDDATKTFTVTE\" if len(starting_seq) == 0 else starting_seq\n",
    "length = len(starting_seq)\n",
    "template = \"None\" #@param [\"custom\",\"None\"]\n",
    "if template == \"custom\":\n",
    "  custom_template_path = os.path.join(template,f\"template\")\n",
    "  os.makedirs(custom_template_path, exist_ok=True)\n",
    "  uploaded = files.upload()\n",
    "  for fn in uploaded.keys():\n",
    "    os.rename(fn,os.path.join(custom_template_path,fn))\n",
    "  template_path = os.path.join(custom_template_path,fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapse": true
   },
   "outputs": [],
   "source": [
    "#@title initialize the model with parameters and run {\"vertical-output\":true,\"form-width\":\"50%\",\"display-mode\":\"form\"}\n",
    "clear_mem()\n",
    "model_name = \"model_1_ptm\" #@param [\"model_1_ptm\", \"model_2_ptm\",\"both\"]\n",
    "use_multimer = False \n",
    "model_name = None if model_name == \"both\" else [model_name]\n",
    "af_model = mk_afdesign_model(protocol=\"hallucination\",\n",
    "                             use_templates=True,\n",
    "                             debug=True, \n",
    "                             model_names=model_name,\n",
    "                             use_multimer=use_multimer)\n",
    "af_model.prep_inputs(length=length)\n",
    "\n",
    "mode = \"dgram\" #@param [\"dgram\",\"dgram_retrain\"]\n",
    "if \"dgram\" in mode:\n",
    "  if \"retrain\" in mode and not use_multimer:\n",
    "    # update distogram head to return all 39 bins\n",
    "    af_model._cfg.model.heads.distogram.first_break = 3.25\n",
    "    af_model._cfg.model.heads.distogram.last_break = 50.75\n",
    "    af_model._cfg.model.heads.distogram.num_bins = 39\n",
    "    af_model._model = af_model._get_model(af_model._cfg)\n",
    "    from colabdesign.af.weights import __file__ as af_path\n",
    "    template_dgram_head = np.load(os.path.join(os.path.dirname(af_path),'template_dgram_head.npy'))\n",
    "    for k in range(len(af_model._model_params)):\n",
    "      params = {\"weights\":jnp.array(template_dgram_head[k]),\"bias\":jnp.zeros(39)}\n",
    "      af_model._model_params[k][\"alphafold/alphafold_iteration/distogram_head/half_logits\"] = params\n",
    "  else:\n",
    "    dgram_map = np.eye(39)[np.repeat(np.append(0,np.arange(15)),4)]\n",
    "    dgram_map[-1,:] = 0 \n",
    "\n",
    "iterations = 50 #@param [50, 100, 200] {type:\"raw\"}\n",
    "use_dgram_noise = None #@param [\"g\",\"u\",\"None\"]\n",
    "use_dropout = False #@param {type:\"boolean\"}\n",
    "seqsep_mask =  0 #@param {type:\"integer\"}\n",
    "num_recycles = 2 #@param {type:\"integer\"}\n",
    "\n",
    "sample_models = True if model_name == \"both\" else False\n",
    "dgram_noise_type = use_dgram_noise\n",
    "use_dgram_noise = False if use_dgram_noise is None else True\n",
    "\n",
    "L = sum(af_model._lengths)\n",
    "af_model.restart(mode=\"gumbel\")\n",
    "af_model._inputs[\"rm_template_seq\"] = False\n",
    "# gather info about inputs\n",
    "if \"offset\" in af_model._inputs:           \n",
    "  offset = af_model._inputs\n",
    "else:\n",
    "  idx = af_model._inputs[\"residue_index\"]\n",
    "  offset = idx[:,None] - idx[None,:]\n",
    "\n",
    "# initialize sequence\n",
    "if len(starting_seq) > 1:\n",
    "  af_model.set_seq(seq=starting_seq)\n",
    "af_model._inputs[\"bias\"] = np.zeros((L,20))\n",
    "\n",
    "# initialize coordinates/dgram\n",
    "af_model._inputs[\"batch\"] = {\"aatype\":np.zeros(L).astype(int),\n",
    "                             \"all_atom_mask\":np.zeros((L,37)),\n",
    "                             \"all_atom_positions\":np.zeros((L,37,3)),\n",
    "                             \"dgram\":np.zeros((L,L,39))}\n",
    "\n",
    "if template == \"custom\":\n",
    "  xyz = xyz_atom37(pdb_file=template_path)\n",
    "  af_model._inputs[\"batch\"][\"all_atom_positions\"] = xyz\n",
    "  dgram = get_dgram(xyz)\n",
    "  mask = np.abs(offset) > seqsep_mask\n",
    "  af_model._inputs[\"batch\"][\"dgram\"] = dgram * mask[:,:,None]\n",
    "  if use_dgram_noise:\n",
    "    if dgram_noise_type == \"g\":   \n",
    "      noise = sample_gumbel(dgram.shape) * (1 - k/iterations)\n",
    "      dgram = softmax(np.log(dgram + 1e-8) + noise, -1)\n",
    "    elif dgram_noise_type == 'u':  \n",
    "      noise = sample_uniform(dgram.shape) * (1 - k/iterations)\n",
    "      dgram = softmax(np.log(dgram + 1e-8) + noise, -1)\n",
    "plddts = []\n",
    "print(f\"running seq {starting_seq} with model: {'both' if model_name is None else model_name} for {iterations} steps\")\n",
    "for k in range(iterations):\n",
    "  # noise\n",
    "  if k > 0:\n",
    "    dgram_xyz = get_dgram(xyz)\n",
    "    dgram_prob = softmax(dgram_logits,-1)\n",
    "\n",
    "    if mode == \"xyz\":\n",
    "      dgram = dgram_xyz\n",
    "    if mode == \"dgram\":\n",
    "      dgram = dgram_prob @ dgram_map\n",
    "      dgram[...,14:] = dgram_xyz[...,14:] * dgram_prob[...,-1:]\n",
    "    if mode == \"dgram_retrain\":\n",
    "      dgram = dgram_prob\n",
    "    \n",
    "    if use_dgram_noise:\n",
    "      if dgram_noise_type == \"g\":   \n",
    "        noise = sample_gumbel(dgram.shape) * (1 - k/iterations)\n",
    "        dgram = softmax(np.log(dgram + 1e-8) + noise, -1)\n",
    "      elif dgram_noise_type == 'u':  \n",
    "        noise = sample_uniform(dgram.shape) * (1 - k/iterations)\n",
    "        dgram = softmax(np.log(dgram + 1e-8) + noise, -1)\n",
    "\n",
    "    # add mask to avoid local contacts being fixed (otherwise there is a bias toward helix)\n",
    "    mask = np.abs(offset) > seqsep_mask\n",
    "    af_model._inputs[\"batch\"][\"dgram\"] = dgram * mask[:,:,None]\n",
    "\n",
    "  # prediction\n",
    "  aux = af_model.predict(return_aux=True, verbose=False,\n",
    "                        sample_models=sample_models,\n",
    "                        dropout=use_dropout, num_recycles=num_recycles)\n",
    "  plddt = aux[\"plddt\"]\n",
    "  plddts.append(np.average(plddt))\n",
    "  seq = aux[\"seq\"][\"hard\"][0].argmax(-1)   \n",
    "  xyz = aux[\"atom_positions\"].copy()\n",
    "  dgram_logits = aux[\"debug\"][\"outputs\"][\"distogram\"][\"logits\"] \n",
    "  \n",
    "  # update inputs    \n",
    "  af_model._inputs[\"batch\"][\"aatype\"] = seq\n",
    "  af_model._inputs[\"batch\"][\"all_atom_mask\"][:,:4] = np.sqrt(plddt)[:,None]\n",
    "  af_model._inputs[\"batch\"][\"all_atom_positions\"] = xyz\n",
    "  \n",
    "  # save results\n",
    "  af_model._save_results(aux)\n",
    "  af_model._k += 1\n",
    "  af_model.save_pdb(f\"iter_{k}.pdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapse": true
   },
   "outputs": [],
   "source": [
    "#@title visualization\n",
    "fig,ax=plt.subplots(1,1,figsize=(7.4,2))\n",
    "ax.scatter(range(len(plddts)),np.array(plddts)*100,s=12, color='grey', zorder=1)\n",
    "ax.plot(np.array(plddts)*100,'darkorange',zorder=0)\n",
    "ax.set_xlabel(\"Prediction iteration\")\n",
    "ax.set_ylabel(\"pLDDT\")\n",
    "ax.text(ax.get_xlim()[0]+(ax.get_xlim()[1]-ax.get_xlim()[0])*0.85,\\\n",
    "        ax.get_ylim()[0]+(ax.get_ylim()[1]-ax.get_ylim()[0])*0.05,f\"recycle# {num_recycles}\")\n",
    "HTML(af_model.animate(dpi=80, interval=300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapse": true
   },
   "outputs": [],
   "source": [
    "#@title Visualize precalculated iterative structure predictions from PDB {\"vertical-output\":true,\"form-width\":\"50%\",\"display-mode\":\"form\"}\n",
    "!pip install plotly\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "#import nglview as nv\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapse": true
   },
   "outputs": [],
   "source": [
    "#@title helpful functions for download data from zenodo and visualization {\"vertical-output\":true,\"form-width\":\"50%\",\"display-mode\":\"form\"}\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import shutil\n",
    "\n",
    "def plot_embedding(embedding,ss,rmsd,pdb_id,rmsd_cutoff=1,selection=None,selection_min=0, selection_max=1):\n",
    "    fig = go.Figure()\n",
    "    # plt.figure(figsize=(20,20))\n",
    "    embedding_df = pd.DataFrame()\n",
    "    embedding_df['dim1'] = np.round(embedding[:, 0],2)\n",
    "    embedding_df['dim2'] = np.round(embedding[:, 1],2)\n",
    "    embedding_df['%alpha'] = np.round(ss[:,3],2)\n",
    "    embedding_df['%beta'] = np.round(ss[:,2],2)\n",
    "    embedding_df['%coil'] = np.round(ss[:,4],2)\n",
    "    embedding_df['RMSD'] = np.round(rmsd,2)*10\n",
    "    embedding_df['pdb_id'] = np.array([i[-5:-1].upper() for i in pdb_id])\n",
    "    if selection:\n",
    "        for key in embedding_df.keys():\n",
    "            embedding_df[key] = embedding_df[key][np.where((embedding_df[selection]>selection_min) & (embedding_df[selection]<selection_max))[0]]\n",
    "    fig = px.scatter(embedding_df,x='dim1',y='dim2',color='%alpha',\n",
    "                     custom_data=[embedding_df['pdb_id'],\n",
    "                                 embedding_df['%alpha'],\n",
    "                                 embedding_df['%beta'],\n",
    "                                 embedding_df['%coil'],\n",
    "                                 embedding_df['RMSD']],\n",
    "                     hover_data={'dim1':False,\n",
    "                                 'dim2':False,\n",
    "                                 'pdb_id':True,\n",
    "                                 '%alpha': True,\n",
    "                                 '%beta': True,\n",
    "                                 '%coil': True,\n",
    "                                 'RMSD': (':.2f')},\n",
    "                    color_continuous_scale='RdBu')\n",
    "    fig.update_layout(\n",
    "    #     margin=dict(l=10, r=10, t=10, b=10),\n",
    "        width=800,height=800,\n",
    "    #     paper_bgcolor=\"LightSteelBlue\",\n",
    "    )\n",
    "    fig.update_traces(marker=dict(size=10,\n",
    "                                  line=dict(width=2,\n",
    "                                            color='DarkSlateGrey')),\n",
    "                      selector=dict(mode='markers'))\n",
    "    # Add dropdown\n",
    "    fig.update_xaxes(showspikes=True,spikecolor=\"black\", spikesnap=\"cursor\", spikemode=\"across\")\n",
    "    fig.update_yaxes(showspikes=True,spikecolor=\"black\", spikesnap=\"cursor\", spikemode=\"across\")\n",
    "    fig.update_layout(\n",
    "    #     xaxis=dict(rangeslider=dict(visible=True)),\n",
    "        updatemenus=[go.layout.Updatemenu(\n",
    "                active=0,\n",
    "                buttons=list([\n",
    "                    dict(\n",
    "                        args=[{\"marker.color\": [embedding_df[\"%alpha\"]]}],\n",
    "                        label=\"helix percentage\",\n",
    "                        method=\"restyle\"\n",
    "                    ),\n",
    "                    dict(\n",
    "                        args=[{\"marker.color\": [embedding_df[\"%beta\"]]}],\n",
    "                        label=\"beta percentage\",\n",
    "                        method=\"restyle\"\n",
    "                    ),\n",
    "                    dict(\n",
    "                        args=[{\"marker.color\": [embedding_df[\"%coil\"]]}],\n",
    "                        label=\"coil percentage\",\n",
    "                        method=\"restyle\"\n",
    "                    ),\n",
    "                    dict(\n",
    "                        args=[{\"marker.color\": [embedding_df[\"RMSD\"]]}],\n",
    "                        label=\"RMSD\",\n",
    "                        method=\"restyle\"\n",
    "                    )\n",
    "                ]),\n",
    "                direction=\"down\",\n",
    "                pad={\"l\": -30, \"t\": 1},\n",
    "                showactive=True,\n",
    "                x=0.1,\n",
    "                xanchor=\"left\",\n",
    "                y=1.1,\n",
    "                yanchor=\"top\"\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    fig.update_traces(hovertemplate=\"PDB: %{customdata[0]}<br> \\u03B1: %{customdata[1]}; \\u03B2: %{customdata[2]}; C: %{customdata[3]}<br> RMSD: %{customdata[4]:.2f} \") #\n",
    "    fig.show(\"notebook\")\n",
    "    # px.data.iris()\n",
    "\n",
    "# Function to download part of a file in small chunks\n",
    "def download_chunk(file_url, start, end, file_name, pbar, chunk_size=1024*1024):\n",
    "    headers = {'Range': f'bytes={start}-{end}'}\n",
    "    response = requests.get(file_url, headers=headers, stream=True)\n",
    "    \n",
    "    with open(file_name, 'r+b') as file:\n",
    "        file.seek(start)\n",
    "        \n",
    "        for chunk in response.iter_content(chunk_size=chunk_size):\n",
    "            if chunk:  # Filter out keep-alive new chunks\n",
    "                file.write(chunk)\n",
    "                pbar.update(len(chunk))\n",
    "\n",
    "# Function to download a file using multiple threads, each handling a part of the file\n",
    "def download_large_file_multithreaded(url, destination, num_threads=4, chunk_size=1024*1024):\n",
    "    # Get the total file size\n",
    "    response = requests.head(url)\n",
    "    total_size = int(response.headers['content-length'])\n",
    "\n",
    "    # Create an empty file with the appropriate size\n",
    "    with open(destination, 'wb') as f:\n",
    "        f.truncate(total_size)\n",
    "\n",
    "    # Define the size for each thread to download\n",
    "    part_size = total_size // num_threads\n",
    "\n",
    "    # Set up progress bar\n",
    "    pbar = tqdm(total=total_size, unit='B', position=0, leave=True, unit_scale=True, desc=destination)\n",
    "\n",
    "    # Create a thread pool for parallel downloads\n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        futures = []\n",
    "        for i in range(num_threads):\n",
    "            start = i * part_size\n",
    "            end = (i + 1) * part_size - 1 if i < num_threads - 1 else total_size - 1\n",
    "            futures.append(executor.submit(download_chunk, url, start, end, destination, pbar, chunk_size))\n",
    "\n",
    "        # Wait for all threads to finish\n",
    "        for future in futures:\n",
    "            future.result()\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "import zipfile\n",
    "import os\n",
    "import threading\n",
    "\n",
    "# Function to extract a single file and update progress bar\n",
    "def extract_file(zip_ref, file_info, destination, progress_bar):\n",
    "    zip_ref.extract(file_info, destination)\n",
    "    progress_bar.update(1)  # Update progress bar after extracting each file\n",
    "\n",
    "# Multi-threaded decompression function with progress bar\n",
    "def extract_zip_multithreaded(zip_path, destination, file_name=None, num_threads=4):\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        file_list = zip_ref.infolist()\n",
    "        if file_name != None:\n",
    "          filelist = []\n",
    "          for file in file_list:\n",
    "            for filename in file_name:\n",
    "              if filename in file.filename:\n",
    "                print(file.filename)\n",
    "                filelist.append(file)\n",
    "          file_list = filelist\n",
    "        \n",
    "        # Create the destination folder if it doesn't exist\n",
    "        os.makedirs(destination, exist_ok=True)\n",
    "\n",
    "        # Initialize tqdm progress bar\n",
    "        with tqdm(total=len(file_list), position=0, leave=True, desc=\"Decompressing\", unit=\"file\") as progress_bar:\n",
    "            # Create threads to extract files in parallel\n",
    "            threads = []\n",
    "            for i in range(num_threads):\n",
    "                # Split the file list for each thread\n",
    "                part_files = file_list[i::num_threads]\n",
    "                for file_info in part_files:\n",
    "                    thread = threading.Thread(target=extract_file, args=(zip_ref, file_info, destination, progress_bar))\n",
    "                    threads.append(thread)\n",
    "                    thread.start()\n",
    "\n",
    "            # Wait for all threads to finish\n",
    "            for thread in threads:\n",
    "                thread.join()\n",
    "\n",
    "\n",
    "def download_zenodo(record_id,download_file=None,pdb=None):\n",
    "\n",
    "    zenodo_url = f'https://zenodo.org/api/records/{record_id}'\n",
    "\n",
    "    # Get record metadata\n",
    "    response = requests.get(zenodo_url)\n",
    "    response.raise_for_status()\n",
    "    metadata = response.json()\n",
    "\n",
    "    # Get all file URLs\n",
    "    files = metadata['files']\n",
    "\n",
    "    # Create a folder to store downloaded files\n",
    "    # if os.path.exists('zenodo_downloads'):\n",
    "    #     shutil.rmtree('zenodo_downloads')\n",
    "    os.makedirs('zenodo_downloads', exist_ok=True)\n",
    "    # Download and decompress each file in the record\n",
    "    for file_info in files:\n",
    "        file_url = file_info['links']['self']\n",
    "        file_name = file_info['key']\n",
    "        if file_name in download_file:\n",
    "            file_path = os.path.join('zenodo_downloads', file_name)\n",
    "            print(f'Downloading {file_name} with memory-efficient multi-threading...')\n",
    "            download_large_file_multithreaded(file_url, file_path, num_threads=4, chunk_size=1024*1024)\n",
    "\n",
    "            # Check if the file is a ZIP file and decompress it\n",
    "            if zipfile.is_zipfile(file_path) and \"trajs\" not in file_path and file_path != '0_0_iter_0_pdb.zip':\n",
    "                print(f'Decompressing {file_name}...')\n",
    "                extract_zip_multithreaded(file_path, 'zenodo_downloads', file_name=pdb, num_threads=4)\n",
    "\n",
    "                # with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
    "                #     zip_ref.extractall('zenodo_downloads')\n",
    "                os.remove(file_path)  # Optionally remove the zip file after extraction\n",
    "            # elif \"trajs\" in file_path or file_path == '0_0_iter_0_pdb.zip':\n",
    "            #     extract_zip_multithreaded(file_path, 'zenodo_downloads', file_name=pdb, num_threads=4)\n",
    "\n",
    "\n",
    "print('Download files useful for both trajectory and embedding visualization.')\n",
    "record_ids = {'6':'13766276','3':'13766281','4':'13777196','2':'13772757','18_and_embeddings':'13826566','57':'13788039'}\n",
    "fig_1_6_pdbs = ['3GB1','1MI0','1HZ5','1KH0','1UBQ','2HDA']\n",
    "\n",
    "download_file = ['rmsds_plddts_embeddings.zip','model_1_2_gap_0_6_per_residue_plddts.zip','0_0_iter_0_pdb.zip']\n",
    "record_id = record_ids['18_and_embeddings']\n",
    "download_zenodo(record_id=record_id,download_file=download_file)\n",
    "\n",
    "print('Download and decompression complete.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapse": true
   },
   "outputs": [],
   "source": [
    "#@title visualize embeddings {\"vertical-output\":true,\"form-width\":\"50%\",\"display-mode\":\"form\"}\n",
    "visualize_embeddings = True #@param {type:\"boolean\"}\n",
    "if visualize_embeddings:\n",
    "    download_file = ['all_ss_gap0.npy', 'all_ss_gap6.npy', 'all_pdbs_gap0.npy', 'all_seq_length_gap0.npy', 'rmsds_plddts_embeddings.zip']\n",
    "    record_id = record_ids['18_and_embeddings']\n",
    "    download_zenodo(record_id=record_id,download_file=download_file)\n",
    "\n",
    "all_ss = np.load(\"./zenodo_downloads/all_ss_gap0.npy\")\n",
    "all_rmsd = np.load(\"./zenodo_downloads/all_rmsd_model1_gap0.npy\")\n",
    "all_rmsd2 = np.load(\"./zenodo_downloads/all_rmsd_model2_gap0.npy\")\n",
    "all_seq_length = np.load(\"./zenodo_downloads/all_seq_length_gap0.npy\")\n",
    "all_rmsd = np.array(all_rmsd)\n",
    "all_rmsd2 = np.array(all_rmsd2)\n",
    "all_tmfile_pd=np.load(\"./zenodo_downloads/all_pdbs_gap0.npy\")\n",
    "all_EH = np.array([[(np.array([*i])=='E').sum()/((np.array([*i])=='E').sum()+(np.array([*i])=='H').sum()),\n",
    "                    (np.array([*i])=='H').sum()/((np.array([*i])=='E').sum()+(np.array([*i])=='H').sum()),\n",
    "                    (np.array([*i])=='E').sum()/len(i),\n",
    "                    (np.array([*i])=='H').sum()/len(i),\n",
    "                    (np.array([*(i.strip('C'))])=='C').sum()/len(i)] for i in all_ss])\n",
    "all_H = [(np.array([*i])=='H').sum()/((np.array([*i])=='E').sum()+(np.array([*i])=='H').sum()) for i in all_ss]\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = 'notebook'\n",
    "import plotly.offline as pyo\n",
    "pyo.init_notebook_mode(connected=True)\n",
    "seq_length_threshold = 100\n",
    "long_seq_index = np.where(all_seq_length>seq_length_threshold)[0]\n",
    "all_X_embedded = np.load('./result/TSE_embedding_gap0.npy')\n",
    "X_embedded = all_X_embedded[9]\n",
    "plot_embedding(X_embedded[long_seq_index],all_EH[long_seq_index],all_rmsd2[long_seq_index],np.array(all_tmfile_pd)[long_seq_index],selection='RMSD',selection_min=0.,selection_max=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapse": true
   },
   "outputs": [],
   "source": [
    "#@title visualize individual pdb {\"vertical-output\":true,\"form-width\":\"50%\",\"display-mode\":\"form\"}\n",
    "pdb_id = '3gb1'.upper() #@param {type:\"str\"}\n",
    "\n",
    "if pdb_id is not None:\n",
    "    if pdb_id in fig_1_6_pdbs:\n",
    "        download_file = f\"{pdb_id.lower()}.zip\"\n",
    "        record_id = record_ids['18_and_embeddings']\n",
    "        download_zenodo(record_id,download_file=download_file)\n",
    "    elif pdb_id[0] in ['6','3','4','2']:\n",
    "        record_id = record_ids[pdb_id[0]]\n",
    "        download_file = [f\"model_1_2_gap_0_6_trajs_{pdb_id[0]}.zip\"]\n",
    "        download_zenodo(record_id,download_file=download_file,pdb=pdb_id)\n",
    "        extract_zip_multithreaded(f'zenodo_downloads/model_1_2_gap_0_6_trajs_{pdb_id[0]}.zip', 'zenodo_downloads', file_name=pdb_id, num_threads=4)\n",
    "        extract_zip_multithreaded('zenodo_downloads/0_0_iter_0_pdb.zip', 'zenodo_downloads', file_name=pdb_id, num_threads=4)\n",
    "        extract_zip_multithreaded('zenodo_downloads/model_1_2_gap_0_6_per_residue_plddts.zip', 'zenodo_downloads', file_name=pdb_id, num_threads=4)\n",
    "    elif pdb_id[0] in ['1','8']:\n",
    "        record_id = record_ids['18_and_embeddings']\n",
    "        download_file = [f\"model_1_2_gap_0_6_trajs_{pdb_id[0]}.zip\"]\n",
    "        download_zenodo(record_id,download_file=download_file,pdb=pdb_id)\n",
    "        extract_zip_multithreaded(f'zenodo_downloads/model_1_2_gap_0_6_trajs_{pdb_id[0]}.zip', 'zenodo_downloads', file_name=pdb_id, num_threads=4)\n",
    "        extract_zip_multithreaded('zenodo_downloads/0_0_iter_0_pdb.zip', 'zenodo_downloads', file_name=pdb_id, num_threads=4)\n",
    "        extract_zip_multithreaded('zenodo_downloads/model_1_2_gap_0_6_per_residue_plddts.zip', 'zenodo_downloads', file_name=pdb_id, num_threads=4)\n",
    "    elif pdb_id[0] in ['5','7']:\n",
    "        record_id = record_ids['57']\n",
    "        download_file = [f\"model_1_2_gap_0_6_trajs_{pdb_id[0]}.zip\"]\n",
    "        download_zenodo(record_id,download_file=download_file,pdb=pdb_id)\n",
    "        extract_zip_multithreaded(f'zenodo_downloads/model_1_2_gap_0_6_trajs_{pdb_id[0]}.zip', 'zenodo_downloads', file_name=pdb_id, num_threads=4)\n",
    "        extract_zip_multithreaded('zenodo_downloads/0_0_iter_0_pdb.zip', 'zenodo_downloads', file_name=pdb_id, num_threads=4)\n",
    "        extract_zip_multithreaded('zenodo_downloads/model_1_2_gap_0_6_per_residue_plddts.zip', 'zenodo_downloads', file_name=pdb_id, num_threads=4)\n",
    "    else:\n",
    "        print(f\"{pdb_id} is not available.\")\n",
    "\n",
    "!pip install mdtraj\n",
    "import glob\n",
    "import mdtraj as md\n",
    "\n",
    "traj = glob.glob(f\"zenodo_downloads/*/{pdb_id}/*xtc\")[0]\n",
    "top = glob.glob(f\"zenodo_downloads/*/{pdb_id}/{pdb_id}*pdb\")[0]\n",
    "traj = md.load(traj,top=top)\n",
    "xyz = traj.xyz\n",
    "seq = sequence_to_one_hot(traj.top.to_fasta()[0])\n",
    "HTML(make_animation(seq, xyz=xyz, pae=None, dpi=80, interval=300))\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMekAuezg/uY2ZS27KOBLN+",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
